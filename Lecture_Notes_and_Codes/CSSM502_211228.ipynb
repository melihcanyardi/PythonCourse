{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce43dc24",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87be2f",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "- Artificial neuron\n",
    "- Developed in 1950's and 60's by Frank Rosenblatt\n",
    "- Perceptrons no longer common — main neuron model is sigmoid neurons — but perceptrons help motivate\n",
    "- Takes several binary inputs and produces a single binary output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98807bc8",
   "metadata": {},
   "source": [
    "## Perceptron Weights\n",
    "\n",
    "- In example, three inputs, $x_1, x_2, x_3$ (could be more or fewer)\n",
    "- Also a weight for each input, $w_1, w_2, w_3$\n",
    "- Real numbers expressing the importance of the respective inputs to the output\n",
    "- Neuron's output, 0 or 1, is determined by whether weighted sum $\\sum_j w_jx_j$ is less than or greater than a threshold\n",
    "- Just like weights, threshold is a real number and a parameter of the neuron\n",
    "\n",
    "$\\text{output} = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if} \\sum_j w_jx_j \\leq threshold\\\\\n",
    "    1 & \\text{if} \\sum_j w_jx_j > threshold\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc98f9",
   "metadata": {},
   "source": [
    "## Perceptron Model\n",
    "\n",
    "- Model decision-making\n",
    "- Larger $w$ for an input the more that factor matters to the ultimate decision\n",
    "- By varying weights and threshold, can get different models of decision-making\n",
    "- A perceptron can weigh up different kinds of evidence in order to make decisions\n",
    "- Complex network of perceptrons could model subtle decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e01992",
   "metadata": {},
   "source": [
    "## Perceptron Model (cont.)\n",
    "\n",
    "- First column of perceptrons — called the first layer of perceptrons — is making three simple decisions by weighing input evidence\n",
    "- Second layer is making a decision by weighing results of first layer\n",
    "- Perceptron in second level can make a decision at a more abstract and complex level\n",
    "- Many-layer network can engage in sophisticated decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb01c2",
   "metadata": {},
   "source": [
    "## Notational Conveniences\n",
    "\n",
    "- Two notational changes to simplify the above\n",
    "- Write as a dot product, $w . x \\equiv \\sum_j w_jx_j$\n",
    "- $w$ and $x$ are now vectors whose components are the weights and inputs\n",
    "- Move the threshold to the other side of the inequality, known as perceptron's bias $\\text{b} \\equiv -threshold$\n",
    "\n",
    "$\\text{output} = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if} w . x \\leq 0\\\\\n",
    "    1 & \\text{if} w . x > 0\n",
    "\\end{cases}$\n",
    "\n",
    "- Bias as a measure of how easy it is to get the perceptron to output a 1, or to fire\n",
    "- Really big bias $\\rightarrow$ easy to get a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd4309",
   "metadata": {},
   "source": [
    "## Elementary Logical Functions\n",
    "\n",
    "- Perceptrons can also be used to compute the elementary logical functions such as AND and OR\n",
    "- Suppose we have perceptron with two inputs, each with weight 􀀀2 and a bias of 3.\n",
    "- Input 00 produces a 1, since $-2*0 -2*0 + 3 > 0$\n",
    "- Input 01 and 10 produce a 1, but 11 produces a 0\n",
    "- We have produced a NAND gate\n",
    "- We can build any computation up from NAND gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944845ab",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "- Suppose we have a network of perceptrons that we'd like to use to learn to solve some problem (e.g., classifying digits)\n",
    "- To see how learning might work, suppose we make a small change in some weight (or bias) in the network\n",
    "- What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network\n",
    "- If it were true that a small change in a weight (or bias) causes only a small change in output, then we could use this fact to modify the weights and biases to get our network to behave more in the manner we want\n",
    "- And then we'd repeat this, changing the weights and biases over and over to produce better and better output $\\rightarrow$ learning\n",
    "- The problem is that this is not what happens in network of perceptrons\n",
    "- A small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip\n",
    "- That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6298e66",
   "metadata": {},
   "source": [
    "## Sigmoid Neuron\n",
    "\n",
    "- Sigmoid neurons are similar to perceptrons, but modifed so that small changes in their weights and bias cause only a small change in their output\n",
    "- Just like a perceptron, the sigmoid neuron has inputs, $x1, x2, ...$\n",
    "- But instead of being just 0 or 1, these inputs can also take on any values between 0 and 1\n",
    "- The output is no longer 0 or 1, it is $\\sigma(w . x + b)$, where $\\sigma$ is the sigmoid function (also called logistic function and logistic neurons)\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\; = \\frac{1}{1 + \\exp(-\\sum_h w_jx_j - b)}$\n",
    "\n",
    "- When very negative, close to 0, when very positive, close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15037245",
   "metadata": {},
   "source": [
    "## Smoothness of the Sigmoid\n",
    "\n",
    "- By using the actual sigmoid function we get a smoothed out perceptron\n",
    "- It is the smoothness of the sigmoid function that is crucial, not its exact form\n",
    "- The smoothness means that small changes in the weights and bias produce a small change in the output\n",
    "\n",
    "$\\Delta \\text{output} \\approx \\sum_j \\frac{\\partial \\text{output}}{\\partial w_j}\\Delta w_j + \\frac{\\partial \\text{output}}{\\partial b}\\Delta b$\n",
    "\n",
    "- In words, $\\Delta \\text{output}$ is a linear function of the changes to weights and biases\n",
    "- This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48803b",
   "metadata": {},
   "source": [
    "## Interpreting Output\n",
    "\n",
    "- One big difference between perceptrons and sigmoid neurons is that sigmoid neurons don't just output 0 or 1 but any real value between 0 and 1\n",
    "- This can be useful, e.g., if we want to use the output value to represent the average intensity of the pixels in an image input to a neural network\n",
    "- Sometimes it can be a nuisance, e.g. the input image is a 9 or not a 9\n",
    "- In practice set up a convention, e.g. if greater than 0.5 indicates a 9, otherwise does not\n",
    "- Be explicit about convention used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1e1d4",
   "metadata": {},
   "source": [
    "## The Architecture of Neural Networks\n",
    "\n",
    "- Leftmost layer is the input layer, with input neurons\n",
    "- Rightmost is output layer containing output neurons\n",
    "- Middle layer is hidden layer, neither inputs nor outputs\n",
    "- Some networks have multiple hidden layers\n",
    "- Confusingly, sometimes multiple layer networks are called multilayer perceptrons or MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e53aac",
   "metadata": {},
   "source": [
    "## Newtork Design\n",
    "\n",
    "- The design of the input and output layers in a network is often straightforward\n",
    "- For example, suppose we're trying to determine whether a handwritten image depicts a 9 or not\n",
    "- A natural way to design the network is to encode the intensities of the image pixels into the input neurons\n",
    "- If the image is a 64 by 64 greyscale image, then we'd have $4096 = 64*64$ input neurons, with the intensities scaled appropriately between 0 and 1\n",
    "- The output layer will contain just a single neuron, with output values of less than 0.5 indicating input image is not a 9, and values greater than 0.5 indicating input image is a 9\n",
    "- While the design of the input and output layers of a neural network is often straightforward, there can be quite an art to the design of the hidden layers\n",
    "- In particular, it's not possible to sum up the design process for the hidden layers with a few simple rules of thumb\n",
    "- Instead, neural networks researchers have developed many design heuristics for the hidden layers, which help people get the behaviour they want out of their nets\n",
    "- For example, such heuristics can be used to help determine how to trade off the number of hidden layers against the time required to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effc561",
   "metadata": {},
   "source": [
    "## Feedforward Neural Newtorks\n",
    "\n",
    "- Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer\n",
    "- Such networks are called feedforward neural networks\n",
    "- This means there are no loops in the network — information is always fed forward, never fed back\n",
    "- If we did have loops, we'd end up with situations where the input to the sigmoid function depended on the output\n",
    "- That'd be hard to make sense of, and so we don't allow such loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1fa26",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "- There are other models of artifcial neural networks in which feedback loops are possible\n",
    "- These models are called recurrent neural networks\n",
    "- The idea in these models is to have neurons which fire for some limited duration of time, before becoming quiescent\n",
    "- That firing can stimulate other neurons, which may fire a little while later, also for a limited duration\n",
    "- That causes still more neurons to fire, and so over time we get a cascade of neurons firing\n",
    "- Loops don't cause problems in such a model, since a neuron's output only affects its input at some later time, not instantaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027cdcf3",
   "metadata": {},
   "source": [
    "## A Simple Network to Classify Handwritten Digits\n",
    "\n",
    "- We can split the problem of recognizing handwritten digits into two sub-problems\n",
    "- First, we'd like a way of breaking an image containing many digits into a sequence of separate images, each containing a single digit\n",
    "- For example, we'd like to break the image into six separate images\n",
    "- We humans solve this segmentation problem with ease, but it's challenging for a computer program to correctly break up the image\n",
    "- Once the image has been segmented, the program then needs to classify each individual digit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d7370",
   "metadata": {},
   "source": [
    "## The Segmentation Problem\n",
    "\n",
    "- We'll focus on writing a program to solve the second problem, that is, classifying individual digits\n",
    "- We do this because it turns out that the segmentation problem is not so diffcult to solve, once you have a good way of classifying individual digits\n",
    "- There are many approaches to solving the segmentation problem\n",
    "- So instead of worrying about segmentation we'll concentrate on developing a neural network which can solve the more interesting and difficult problem, namely, recognizing individual handwritten digits\n",
    "- To recognize individual digits we will use a three-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf033a8",
   "metadata": {},
   "source": [
    "## The Input Layer\n",
    "\n",
    "- The input layer of the network contains neurons encoding the values of the input pixels\n",
    "- Our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains $784 = 28*28$ neurons\n",
    "- For simplicity I've omitted most of the 784 input neurons in the diagram above\n",
    "- The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fab17",
   "metadata": {},
   "source": [
    "## The Hidden Layer\n",
    "\n",
    "- The second layer of the network is a hidden layer\n",
    "- We denote the number of neurons in this hidden layer by $n$, and we'll experiment with different values for $n$\n",
    "- The example shown illustrates a small hidden layer, containing just $n = 15$ neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adbcaf",
   "metadata": {},
   "source": [
    "## The Output Layer\n",
    "\n",
    "- The output layer of the network contains 10 neurons\n",
    "- If the first neuron fires, i.e., has an output $\\approx$ 1, then that will indicate that the network thinks the digit is a 0\n",
    "- If the second neuron fires then that will indicate that the network thinks the digit is a 1, and so on\n",
    "- A little more precisely, we number the output neurons from 0 through 9, and figure out which neuron has the highest activation value\n",
    "- If that neuron is, say, neuron number 6, then our network will guess that the input digit was a 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
